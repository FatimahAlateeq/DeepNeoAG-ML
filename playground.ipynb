{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebc0862",
   "metadata": {},
   "source": [
    "### <center> Prepare Environment </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd53c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python envirement version: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "h5py            Installed: 3.11.0     Expected: 3.11.0     ✅\n",
      "tqdm            Installed: 4.66.4     Expected: 4.66.4     ✅\n",
      "numpy           Installed: 1.26.4     Expected: 1.26.4     ✅\n",
      "scikit-learn    Installed: 1.4.2      Expected: 1.4.2      ✅\n",
      "tensorflow      Installed: 2.10.1     Expected: 2.10.1     ✅\n",
      "transformers    Installed: 4.40.1     Expected: 4.40.1     ✅\n",
      "tape-proteins   Installed: 0.5        Expected: 0.5        ✅\n",
      "torch           Installed: 2.3.0      Expected: 2.3.0      ✅\n",
      "fair-esm        Installed: 2.0.0      Expected: 2.0.0      ✅\n"
     ]
    }
   ],
   "source": [
    "# Check requirements\n",
    "\n",
    "import pkg_resources\n",
    "import sys\n",
    "\n",
    "required = {\n",
    "    \"h5py\": \"3.11.0\",\n",
    "    \"tqdm\": \"4.66.4\",\n",
    "    \"numpy\": \"1.26.4\",\n",
    "    \"scikit-learn\": \"1.4.2\",\n",
    "    \"tensorflow\": \"2.10.1\",\n",
    "    \"transformers\": \"4.40.1\",\n",
    "    \"tape-proteins\": \"0.5\",\n",
    "    \"torch\": \"2.3.0\",\n",
    "    \"fair-esm\": \"2.0.0\"\n",
    "}\n",
    "print('python envirement version:',sys.version)\n",
    "for pkg, expected_version in required.items():\n",
    "    try:\n",
    "        installed_version = pkg_resources.get_distribution(pkg).version\n",
    "        match = installed_version == expected_version\n",
    "        print(f\"{pkg:<15} Installed: {installed_version:<10} Expected: {expected_version:<10} {'✅' if match else '❌'}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{pkg:<15} Not installed ❌\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5py ✅ 3.11.0\n",
      "tqdm ✅ 4.66.4\n",
      "numpy ✅ 1.26.4\n",
      "scikit-learn ✅ 1.4.2\n",
      "tensorflow ✅ 2.10.1\n",
      "transformers ✅ 4.40.1\n",
      "tape-proteins ✅ 0.5\n",
      "torch not found. Installing 2.3.0...\n",
      "fair-esm not found. Installing 2.0.0...\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "\n",
    "# import subprocess\n",
    "# import sys\n",
    "# import pkg_resources\n",
    "\n",
    "# required = {\n",
    "#     \"h5py\": \"3.11.0\",\n",
    "#     \"tqdm\": \"4.66.4\",\n",
    "#     \"numpy\": \"1.26.4\",\n",
    "#     \"scikit-learn\": \"1.4.2\",\n",
    "#     \"tensorflow\": \"2.10.1\",\n",
    "#     \"transformers\": \"4.40.1\",\n",
    "#     \"tape-proteins\": \"0.5\",\n",
    "#     \"torch\": \"2.3.0\",\n",
    "#     \"fair-esm\": \"2.0.0\"\n",
    "# }\n",
    "\n",
    "# def install_package(pkg, version):\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={version}\"])\n",
    "\n",
    "# for pkg, expected_version in required.items():\n",
    "#     try:\n",
    "#         installed_version = pkg_resources.get_distribution(pkg).version\n",
    "#         if installed_version != expected_version:\n",
    "#             print(f\"Updating {pkg} from {installed_version} to {expected_version}...\")\n",
    "#             install_package(pkg, expected_version)\n",
    "#         else:\n",
    "#             print(f\"{pkg} ✅ {installed_version}\")\n",
    "#     except pkg_resources.DistributionNotFound:\n",
    "#         print(f\"{pkg} not found. Installing {expected_version}...\")\n",
    "#         install_package(pkg, expected_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576bc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# !pip install huggingface_hub[hf_xet]\n",
    "# !pip install sentencepiece\n",
    "\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "# import h5py\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb70096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costume\n",
    "class_p_n='neg'       # neg/pos\n",
    "class_tr_te='test'     # train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d692393",
   "metadata": {},
   "source": [
    "## <center> Preparing Embeddings with pre-Trained Models </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ac98b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input=fr'fasta\\{class_tr_te}\\{class_p_n}'\n",
    "\n",
    "path_output=fr'{class_tr_te}\\{class_p_n}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e52e8",
   "metadata": {},
   "source": [
    "### Model: ProtTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd7b1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a FASTA file and return sequence as a list of tuples\n",
    "def read_fasta(fasta_path):\n",
    "    seq = ''\n",
    "    with open(fasta_path, 'r') as fasta_f:\n",
    "        for line in fasta_f:\n",
    "            if not line.startswith('>'):\n",
    "                seq += line.strip()\n",
    "\n",
    "    seq_id = os.path.splitext(os.path.basename(fasta_path))[0]  # Extract filename without str\n",
    "    seqs = [(seq_id, seq)]\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02969087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ProtTrans T5 model and tokenizer\n",
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device).eval()   # move model to GPU # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ccf3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for protein sequences\n",
    "def get_embeddings(model, tokenizer, seqs, max_residues=4000, max_seq_len=1000, max_batch=100):\n",
    "    results = {\"residue_embs\": dict()}\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
    "    seq_dict = sorted(seqs, key=lambda x: len(x[1]), reverse=True)  # Sort by length for efficient batching\n",
    "    start = time.time()\n",
    "    batch = []\n",
    "\n",
    "    for seq_idx, (pdb_id, seq) in enumerate(seq_dict, 1):\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))  # Add spaces between amino acids\n",
    "        batch.append((pdb_id, seq, seq_len))\n",
    "\n",
    "        # Check if batch size or residue count exceeds limits\n",
    "        # count residues in current batch and add the last sequence length to\n",
    "        # avoid that batches with (n_res_batch > max_residues) get processed\n",
    "        n_res_batch = sum([s_len for _, _, s_len in batch]) + seq_len\n",
    "        if len(batch) >= max_batch or n_res_batch >= max_residues or seq_idx == len(seq_dict) or seq_len > max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = []\n",
    "\n",
    "            # Tokenize sequences\n",
    "            # add_special_tokens adds extra token at the end of each sequence\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "            input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "            except RuntimeError:\n",
    "                print(f\"RuntimeError during embedding for {pdb_id} (L={seq_len})\")\n",
    "                continue\n",
    "\n",
    "            # Extract embeddings for each sequence\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
    "                s_len = seq_lens[batch_idx]\n",
    "                # slice off padding --> batch-size x seq_len x embedding_dim\n",
    "                emb = embedding_repr.last_hidden_state[batch_idx, :s_len]\n",
    "                # if \"residue_embs\" in results:\n",
    "                results[\"residue_embs\"][identifier] = emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "    passed_time = time.time() - start\n",
    "    avg_time = passed_time / len(results[\"residue_embs\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dce31b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_T5_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05d662ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to a text file\n",
    "def save_port_map(port_data, output_file):\n",
    "    np.savetxt(output_file, port_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb17ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process a single FASTA file\n",
    "def main(fasta_file, output_file):\n",
    "    filename = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "    seqs = read_fasta(fasta_file)\n",
    "    results = get_embeddings(model, tokenizer, seqs)\n",
    "    embeddings = results['residue_embs'][filename]\n",
    "    save_port_map(embeddings, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339af633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point: process all .fasta files in the input directory\n",
    "start = time.time()\n",
    "input_files = os.listdir(path_input)\n",
    "str = \".fasta\"\n",
    "for i in input_files:\n",
    "    if i.endswith(str):\n",
    "        file_name = \"/\" + i.split(\".\")[0]\n",
    "        # print( \"/\" + i)\n",
    "        main(path_input + \"/\" + i, path_output + file_name + \".prottrans\")\n",
    "end=time.time()\n",
    "print(\"Elapsed time:\",(end-start)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c74840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a .prottrans file\n",
    "with open(r\"Train\\pos\\neo_sequence24.prottrans\", \"rb\") as f:\n",
    "     for line in f.readlines():\n",
    "        print(line.decode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03630c3",
   "metadata": {},
   "source": [
    "## <center> Get Dataset </center>\n",
    "*After Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99ba4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to input directory containing feature files\n",
    "path_input= path_output     # path_output=fr'{class_tr_te}\\{class_p_n}' From the previous section!\n",
    "# Path to save the processed dataset\n",
    "path_output=fr'dataset\\{class_tr_te}\\{class_p_n}'\n",
    "# File extension/type of input features (e.g., .npy, .esm)\n",
    "data_type='.prottrans'\n",
    "# Maximum number of residues to keep per sequence\n",
    "max_sequence=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ccb7a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load plain text feature data (e.g., .prottrans or .esm)\n",
    "def loadData(path):\n",
    "    Data = np.loadtxt(path)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e9ea0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load binary NumPy feature data (e.g., .npy from TAPE)\n",
    "def loadData_tape(path):\n",
    "    Data = np.load(path)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d367c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final concatenated dataset as a .npy file\n",
    "def saveData(path, data):\n",
    "    print(data.shape)  # Print shape for verification\n",
    "    np.save(path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "483669b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features from ProtTrans or ESM format\n",
    "def get_series_feature(org_data, maxseq, length):\n",
    "    data = np.zeros((maxseq, length), dtype=np.float16)  # Initialize zero-padded array\n",
    "    data_len = len(org_data)\n",
    "    if data_len < maxseq:\n",
    "        data[:data_len, :] = org_data\n",
    "    else:\n",
    "        data[:, :] = org_data[:maxseq, :]\n",
    "    data = data.reshape((1, 1, maxseq, length))  # Reshape for model input\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9de064b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features from TAPE format (stored as [1, seq_len, dim])\n",
    "def get_series_feature_tape(org_data, maxseq, length):\n",
    "    data = np.zeros((maxseq, length), dtype=np.float16)\n",
    "    data_len = len(org_data[0])\n",
    "    if data_len < maxseq:\n",
    "        data[:data_len, :] = org_data[0]\n",
    "    else:\n",
    "        data[:, :] = org_data[0][:maxseq, :]\n",
    "    data = data.reshape((1, 1, maxseq, length))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "05ce424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function\n",
    "def main(path_input, path_output, data_type, maxseq, length):\n",
    "    result = []  # List to collect reshaped feature arrays\n",
    "    input_files = os.listdir(path_input)  # List all files in input directory\n",
    "    for i in input_files:\n",
    "        if i.endswith(data_type):  # Filter files by extension\n",
    "            file_name = i.split(\".\")[0]  # Remove extension\n",
    "            if data_type == \".npy\":\n",
    "                data = loadData_tape(os.path.join(path_input, file_name + data_type))\n",
    "                result.append(get_series_feature_tape(data, maxseq, length))\n",
    "            else:\n",
    "                data = loadData(os.path.join(path_input, file_name + data_type))\n",
    "                result.append(get_series_feature(data, maxseq, length))\n",
    "    data = np.concatenate(result, axis=0)  # Combine all samples\n",
    "    saveData(path_output, data)  # Save final dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51c79a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1343, 1, 35, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Entry point\n",
    "path_input = path_input\n",
    "path_output = path_output\n",
    "data_type = data_type\n",
    "maxseq = max_sequence\n",
    "\n",
    "# Set feature vector length based on model type\n",
    "if data_type == \".prottrans\":\n",
    "    length = 1024\n",
    "elif data_type == \".esm\":\n",
    "    length = 1280\n",
    "elif data_type == \".npy\":\n",
    "    length = 768\n",
    "else:\n",
    "    length = 20  # Default fallback\n",
    "\n",
    "main(path_input, path_output, data_type, maxseq, length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e367b",
   "metadata": {},
   "source": [
    "## <center> Loading Data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59970676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.__version__): 2.10.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "print('(tf.__version__):',tf.__version__)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac9361ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCNN_data_load():\n",
    "    \"\"\"\n",
    "    Load training and testing data for the MCNN model.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tuple containing training data (x_train, y_train) and testing data (x_test, y_test).\n",
    "    \"\"\"\n",
    "    # Define the paths to the training and testing data files\n",
    "    # path_train_pos = \"../dataset/Train/pos.npy\"\n",
    "    # path_train_neg = \"../dataset/Train/neg.npy\"\n",
    "    # path_test_pos = \"../dataset/Test/pos.npy\"\n",
    "    # path_test_neg = \"../dataset/Test/neg.npy\"\n",
    "    path_train_pos = \"dataset/Train/pos.npy\"\n",
    "    path_train_neg = \"dataset/Train/neg.npy\"\n",
    "    path_test_pos = \"dataset/Test/pos.npy\"\n",
    "    path_test_neg = \"dataset/Test/neg.npy\"\n",
    "\n",
    "    # Load the training and testing data using the data_load function\n",
    "    x_train,y_train=data_load(path_train_pos,path_train_neg)\n",
    "    x_test,y_test=data_load(path_test_pos,path_test_neg)\n",
    "    \n",
    "    return(x_train,y_train,x_test,y_test)\n",
    "\n",
    "def data_load(folder1,folder2):\n",
    "    \"\"\"\n",
    "    Load data from two folders and create labels.\n",
    "    \n",
    "    Args:\n",
    "        folder1 (str): Path to the first data folder.\n",
    "        folder2 (str): Path to the second data folder.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tuple containing concatenated data (x) and one-hot encoded labels (y).\n",
    "    \"\"\"\n",
    "    # Load data from the specified folders\n",
    "    f1=np.load(folder1)\n",
    "    f2=np.load(folder2)\n",
    "    \n",
    "    # Create labels for the data\n",
    "    label1 = np.ones(f1.shape[0])\n",
    "    label2 = np.zeros(f2.shape[0])\n",
    "    \n",
    "     # Concatenate the data from both folders\n",
    "    x=np.concatenate([f1,f2], axis=0)\n",
    "    \n",
    "    # Concatenate the labels\n",
    "    y=np.concatenate([label1,label2], axis=0)\n",
    "    \n",
    "    # Convert the labels to one-hot encoding\n",
    "    y= tf.keras.utils.to_categorical(y,2)\n",
    "    \n",
    "    # Collect garbage to free up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    return x ,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f95cc",
   "metadata": {},
   "source": [
    "## <center> Main </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ce717bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install protobuf==3.20.*\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from time import gmtime, strftime\n",
    "import math\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from tensorflow.keras import layers,Model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"code/\")\n",
    "# import loading_data as load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a76b73",
   "metadata": {},
   "source": [
    "### setting of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5b355504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCNN_MC\n",
      "\n",
      "The setting of sequence length:  35\n",
      "The number of filters in the convolutional layer:  64\n",
      "The number of hidden units in the dense layer:  256\n",
      "The batch size for training the model:  256\n",
      "The window sizes for convolutional filters:  [4, 8, 16]\n",
      "The validation mode:  cross\n",
      "The type of data:  prottrans\n",
      "The number of data feature dimensions:  1024\n"
     ]
    }
   ],
   "source": [
    "MAXSEQ = 35         #The setting of sequence length.\n",
    "\n",
    "DATA_TYPE = \"prottrans\"         #The type of data. Options are \"ProtTrans\", \"tape\", \"esm2\", \"esm1b\".\n",
    "\n",
    "NUM_FEATURE = 1024          #\"The number of data feature dimensions. 1024 for ProtTrans, 768 for tape, 1280 for esm2 and esm1b.\"\n",
    "\n",
    "NUM_FILTER = 64         #The number of filters in the convolutional layer.\n",
    "\n",
    "NUM_HIDDEN = 256            #The number of hidden units in the dense layer.\n",
    "\n",
    "BATCH_SIZE  = 256       #The batch size for training the model.\n",
    "\n",
    "WINDOW_SIZES = [4,8,16]     #The window sizes for convolutional filters.\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = ['Negative','Positive']       #The label of dataset.\n",
    "\n",
    "EPOCHS      = 50        #The number of epochs for training the model.\n",
    "\n",
    "K_Fold = 5      #The number of n-fold cross validation.\n",
    "\n",
    "VALIDATION_MODE=\"cross\"     #The validation mode. Options are \"cross\", \"independent\".\n",
    "\n",
    "print(\"\\nMCNN_MC\\n\")\n",
    "print(\"The setting of sequence length: \",MAXSEQ)\n",
    "print(\"The number of filters in the convolutional layer: \",NUM_FILTER)\n",
    "print(\"The number of hidden units in the dense layer: \",NUM_HIDDEN)\n",
    "print(\"The batch size for training the model: \",BATCH_SIZE)\n",
    "print(\"The window sizes for convolutional filters: \",WINDOW_SIZES)\n",
    "print(\"The validation mode: \",VALIDATION_MODE)\n",
    "print(\"The type of data: \",DATA_TYPE)\n",
    "print(\"The number of data feature dimensions: \",NUM_FEATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741359c7",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01a1c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit batch funtion\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, labels, batch_size):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        return np.array(batch_data), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46cb9f",
   "metadata": {},
   "source": [
    "### MCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b3066b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=[32],\n",
    "                 num_filters=256,\n",
    "                 num_hidden=1000):\n",
    "        # Initialize the parent class\n",
    "        super(DeepScan, self).__init__()\n",
    "        # Initialize the input layer\n",
    "        self.input_layer = tf.keras.Input(input_shape)\n",
    "        # Initialize convolution window sizes\n",
    "        self.window_sizes = window_sizes\n",
    "        # Initialize lists to store convolution, pooling, and flatten layers\n",
    "        self.conv2d = []\n",
    "        self.maxpool = []\n",
    "        self.flatten = []\n",
    "        # Create corresponding convolution, pooling, and flatten layers for each window size\n",
    "        for window_size in self.window_sizes:\n",
    "            self.conv2d.append(\n",
    "                layers.Conv2D(filters=num_filters,\n",
    "                              kernel_size=(1, window_size),\n",
    "                              activation=tf.nn.relu,\n",
    "                              padding='valid',\n",
    "                              bias_initializer=tf.constant_initializer(0.1),\n",
    "                              kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
    "            )\n",
    "            self.maxpool.append(\n",
    "                layers.MaxPooling2D(pool_size=(1, MAXSEQ - window_size + 1),\n",
    "                                    strides=(1, MAXSEQ),\n",
    "                                    padding='valid')\n",
    "            )\n",
    "            self.flatten.append(\n",
    "                layers.Flatten()\n",
    "            )\n",
    "        # Initialize Dropout layer to prevent overfitting\n",
    "        self.dropout = layers.Dropout(rate=0.7)\n",
    "        # Initialize the first fully connected layer\n",
    "        self.fc1 = layers.Dense(num_hidden,\n",
    "                                activation=tf.nn.relu,\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "        )\n",
    "        # Initialize the output layer with softmax activation\n",
    "        self.fc2 = layers.Dense(NUM_CLASSES,\n",
    "                                activation='softmax',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "        # Get the output layer by calling the call method\n",
    "        self.out = self.call(self.input_layer)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # List to store outputs of convolution, pooling, and flatten layers\n",
    "        _x = []\n",
    "        # Perform convolution, pooling, and flatten operations for each window size\n",
    "        for i in range(len(self.window_sizes)):\n",
    "            x_conv = self.conv2d[i](x)\n",
    "            x_maxp = self.maxpool[i](x_conv)\n",
    "            x_flat = self.flatten[i](x_maxp)\n",
    "            _x.append(x_flat)\n",
    "        # Concatenate the outputs of all flatten layers\n",
    "        x = tf.concat(_x, 1)\n",
    "        # Apply Dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Apply the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        # Apply the output layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7919c0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3bf71ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training dataset : (5608, 1, 35, 1024)\n",
      "The data type of training dataset : float16\n",
      "The shape of training label : (5608, 2)\n",
      "The shape of validation dataset : (1404, 1, 35, 1024)\n",
      "The data type of validation dataset : float16\n",
      "The shape of validation label : (1404, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test= MCNN_data_load()\n",
    "\n",
    "print(\"The shape of training dataset :\",x_train.shape)\n",
    "print(\"The data type of training dataset :\",x_train.dtype)\n",
    "print(\"The shape of training label :\",y_train.shape)\n",
    "print(\"The shape of validation dataset :\",x_test.shape)\n",
    "print(\"The data type of validation dataset :\",x_test.dtype)\n",
    "print(\"The shape of validation label :\",y_test.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a655c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, x_test, y_test):\n",
    "    \n",
    "    # Generate predictions for the test data\n",
    "    pred_test = model.predict(x_test)\n",
    "    \n",
    "    # Calculate the false positive rate, true positive rate, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:, 1], pred_test[:, 1])\n",
    "    # Calculate the Area Under the Curve (AUC) for the ROC curve\n",
    "    AUC = metrics.auc(fpr, tpr)\n",
    "    # Display the ROC curve\n",
    "    if (VALIDATION_MODE!=\"cross\"):\n",
    "        display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=AUC, estimator_name='mCNN')\n",
    "        display.plot()\n",
    "    \n",
    "    # Calculate the geometric mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    # Locate the index of the largest geometric mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print(f'\\nBest Threshold={thresholds[ix]}, G-Mean={gmeans[ix]}')\n",
    "    # Set the threshold to the one with the highest geometric mean\n",
    "    threshold = thresholds[ix]\n",
    "    # Generate binary predictions based on the threshold\n",
    "    y_pred = (pred_test[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix values: TN, FP, FN, TP\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y_test[:, 1], y_pred).ravel()\n",
    "    # Calculate Sensitivity (Recall)\n",
    "    Sens = TP / (TP + FN) if TP + FN > 0 else 0.0\n",
    "    # Calculate Specificity\n",
    "    Spec = TN / (FP + TN) if FP + TN > 0 else 0.0\n",
    "    # Calculate Accuracy\n",
    "    Acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) if TP + FP > 0 and FP + TN > 0 and TP + FN and TN + FN else 0.0\n",
    "    # Calculate F1 Score\n",
    "    F1 = 2 * TP / (2 * TP + FP + FN)\n",
    "    # Calculate Precision\n",
    "    Prec = TP / (TP + FP)\n",
    "    # Calculate Recall\n",
    "    Recall = TP / (TP + FN)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}, F1={F1:.4f}, Prec={Prec:.4f}, Recall={Recall:.4f}\\n')\n",
    "    \n",
    "    # Return the performance metrics\n",
    "    return TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb930b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4486, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4486, 2)\n",
      "The shape of validation dataset of cross validation: (1122, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1122, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 1, 1, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 12s 652ms/step - loss: 0.7953 - accuracy: 0.9006\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 11s 620ms/step - loss: 0.2613 - accuracy: 0.9565\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 11s 617ms/step - loss: 0.2346 - accuracy: 0.9565\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 11s 625ms/step - loss: 0.1793 - accuracy: 0.9565\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 11s 612ms/step - loss: 0.2151 - accuracy: 0.9565\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 11s 603ms/step - loss: 0.1737 - accuracy: 0.9568\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 11s 608ms/step - loss: 0.1581 - accuracy: 0.9572\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 13s 726ms/step - loss: 0.1479 - accuracy: 0.9585\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 10s 589ms/step - loss: 0.1646 - accuracy: 0.9601\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 11s 603ms/step - loss: 0.2071 - accuracy: 0.9588\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 12s 634ms/step - loss: 0.2837 - accuracy: 0.9592\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 11s 630ms/step - loss: 0.1802 - accuracy: 0.9603\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 11s 625ms/step - loss: 0.1413 - accuracy: 0.9632\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 11s 623ms/step - loss: 0.1269 - accuracy: 0.9628\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 11s 629ms/step - loss: 0.1382 - accuracy: 0.9637\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 11s 621ms/step - loss: 0.1540 - accuracy: 0.9630\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 11s 618ms/step - loss: 0.1145 - accuracy: 0.9668\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 11s 618ms/step - loss: 0.1200 - accuracy: 0.9670\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 11s 622ms/step - loss: 0.1322 - accuracy: 0.9663\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 11s 624ms/step - loss: 0.0990 - accuracy: 0.9695\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 11s 636ms/step - loss: 0.1373 - accuracy: 0.9681\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 12s 637ms/step - loss: 0.1307 - accuracy: 0.9688\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 12s 643ms/step - loss: 0.0868 - accuracy: 0.9704\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 12s 639ms/step - loss: 0.1138 - accuracy: 0.9683\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 11s 637ms/step - loss: 0.1064 - accuracy: 0.9699\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 11s 635ms/step - loss: 0.0951 - accuracy: 0.9712\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 12s 641ms/step - loss: 0.0758 - accuracy: 0.9739\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 12s 654ms/step - loss: 0.1268 - accuracy: 0.9697\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 12s 648ms/step - loss: 0.0912 - accuracy: 0.9753\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 12s 660ms/step - loss: 0.0922 - accuracy: 0.9715\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 0.0878 - accuracy: 0.9726\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 12s 657ms/step - loss: 0.0734 - accuracy: 0.9750\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 0.1052 - accuracy: 0.9701\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 12s 634ms/step - loss: 0.0793 - accuracy: 0.9761\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 11s 633ms/step - loss: 0.0772 - accuracy: 0.9768\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 12s 638ms/step - loss: 0.0847 - accuracy: 0.9753\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 12s 644ms/step - loss: 0.0953 - accuracy: 0.9715\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 11s 609ms/step - loss: 0.0812 - accuracy: 0.9764\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 11s 612ms/step - loss: 0.0721 - accuracy: 0.9768\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 11s 612ms/step - loss: 0.0587 - accuracy: 0.9808\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 11s 605ms/step - loss: 0.0643 - accuracy: 0.9784\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 11s 606ms/step - loss: 0.0564 - accuracy: 0.9817\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 11s 623ms/step - loss: 0.0632 - accuracy: 0.9779\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.0728 - accuracy: 0.9748\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 11s 623ms/step - loss: 0.0937 - accuracy: 0.9739\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 11s 628ms/step - loss: 0.0621 - accuracy: 0.9786\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 11s 627ms/step - loss: 0.0863 - accuracy: 0.9750\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 11s 620ms/step - loss: 0.0506 - accuracy: 0.9822\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 12s 640ms/step - loss: 0.0512 - accuracy: 0.9837\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 11s 635ms/step - loss: 0.0834 - accuracy: 0.9755\n",
      "36/36 [==============================] - 1s 19ms/step\n",
      "\n",
      "Best Threshold=0.003767861519008875, G-Mean=0.8867503412861666\n",
      "TP=40, FP=103, TN=973, FN=6, Sens=0.8696, Spec=0.9043, Acc=0.9029, MCC=0.4601, AUC=0.9301, F1=0.4233, Prec=0.2797, Recall=0.8696\n",
      "\n",
      "2 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4486, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4486, 2)\n",
      "The shape of validation dataset of cross validation: (1122, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1122, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 10s 530ms/step - loss: 0.6167 - accuracy: 0.9505\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 10s 559ms/step - loss: 0.2436 - accuracy: 0.9574\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 0.2289 - accuracy: 0.9574\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 0.1865 - accuracy: 0.9574\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 0.1549 - accuracy: 0.9581\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 10s 564ms/step - loss: 0.2381 - accuracy: 0.9581\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 12s 653ms/step - loss: 0.1512 - accuracy: 0.9628\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 0.1399 - accuracy: 0.9641\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 10s 551ms/step - loss: 0.2252 - accuracy: 0.9621\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 10s 572ms/step - loss: 0.1253 - accuracy: 0.9697\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 10s 558ms/step - loss: 0.1213 - accuracy: 0.9679\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 10s 579ms/step - loss: 0.1177 - accuracy: 0.9697\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 10s 569ms/step - loss: 0.1270 - accuracy: 0.9695\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 10s 566ms/step - loss: 0.1188 - accuracy: 0.9699\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 11s 601ms/step - loss: 0.1101 - accuracy: 0.9737\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 12s 656ms/step - loss: 0.0909 - accuracy: 0.9744\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.1698 - accuracy: 0.9701\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 11s 617ms/step - loss: 0.1181 - accuracy: 0.9712\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 11s 628ms/step - loss: 0.0942 - accuracy: 0.9733\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 11s 624ms/step - loss: 0.1029 - accuracy: 0.9744\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.1164 - accuracy: 0.9708\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 11s 604ms/step - loss: 0.1264 - accuracy: 0.9741\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 11s 615ms/step - loss: 0.1637 - accuracy: 0.9704\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 11s 609ms/step - loss: 0.0827 - accuracy: 0.9770\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 12s 698ms/step - loss: 0.0724 - accuracy: 0.9786\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 12s 667ms/step - loss: 0.0969 - accuracy: 0.9757\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 12s 685ms/step - loss: 0.0658 - accuracy: 0.9824\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 13s 716ms/step - loss: 0.0612 - accuracy: 0.9842\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 11s 615ms/step - loss: 0.0900 - accuracy: 0.9775\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 12s 637ms/step - loss: 0.0616 - accuracy: 0.9853\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 11s 618ms/step - loss: 0.0718 - accuracy: 0.9828\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 11s 625ms/step - loss: 0.0615 - accuracy: 0.9835\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 11s 616ms/step - loss: 0.0709 - accuracy: 0.9799\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 12s 654ms/step - loss: 0.0498 - accuracy: 0.9877\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 12s 676ms/step - loss: 0.0538 - accuracy: 0.9844\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 10s 526ms/step - loss: 0.0600 - accuracy: 0.9862\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 10s 569ms/step - loss: 0.0442 - accuracy: 0.9891\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 11s 587ms/step - loss: 0.0716 - accuracy: 0.9808\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 10s 557ms/step - loss: 0.0468 - accuracy: 0.9880\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 0.0415 - accuracy: 0.9889\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 0.0639 - accuracy: 0.9831\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 10s 527ms/step - loss: 0.0420 - accuracy: 0.9906\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 0.0359 - accuracy: 0.9897\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 11s 594ms/step - loss: 0.0674 - accuracy: 0.9822\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 11s 591ms/step - loss: 0.0469 - accuracy: 0.9880\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 12s 646ms/step - loss: 0.0397 - accuracy: 0.9891\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 12s 661ms/step - loss: 0.0580 - accuracy: 0.9833\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 12s 668ms/step - loss: 0.0417 - accuracy: 0.9900\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 12s 657ms/step - loss: 0.0362 - accuracy: 0.9897\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 12s 674ms/step - loss: 0.0456 - accuracy: 0.9880\n",
      "36/36 [==============================] - 1s 23ms/step\n",
      "\n",
      "Best Threshold=0.0009942122269421816, G-Mean=0.8474390745502752\n",
      "TP=39, FP=85, TN=987, FN=11, Sens=0.7800, Spec=0.9207, Acc=0.9144, MCC=0.4612, AUC=0.8901, F1=0.4483, Prec=0.3145, Recall=0.7800\n",
      "\n",
      "3 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4486, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4486, 2)\n",
      "The shape of validation dataset of cross validation: (1122, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1122, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 12s 639ms/step - loss: 0.8556 - accuracy: 0.9191\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 10s 573ms/step - loss: 0.3615 - accuracy: 0.9561\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 10s 575ms/step - loss: 0.2434 - accuracy: 0.9561\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 11s 603ms/step - loss: 0.2217 - accuracy: 0.9561\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 11s 611ms/step - loss: 0.1752 - accuracy: 0.9565\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 10s 576ms/step - loss: 0.1968 - accuracy: 0.9563\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 0.1582 - accuracy: 0.9572\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 10s 528ms/step - loss: 0.2408 - accuracy: 0.9568\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 9s 522ms/step - loss: 0.1683 - accuracy: 0.9594\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 0.2188 - accuracy: 0.9588\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.1244 - accuracy: 0.9610\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.1625 - accuracy: 0.9623\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 11s 608ms/step - loss: 0.1221 - accuracy: 0.9652\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 10s 538ms/step - loss: 0.1384 - accuracy: 0.9652\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 9s 519ms/step - loss: 0.1258 - accuracy: 0.9672\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 0.1304 - accuracy: 0.9659\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 0.1221 - accuracy: 0.9679\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 9s 513ms/step - loss: 0.1211 - accuracy: 0.9688\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 9s 519ms/step - loss: 0.1305 - accuracy: 0.9675\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 10s 531ms/step - loss: 0.1144 - accuracy: 0.9690\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 12s 657ms/step - loss: 0.1046 - accuracy: 0.9710\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 11s 638ms/step - loss: 0.0962 - accuracy: 0.9735\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 11s 619ms/step - loss: 0.1081 - accuracy: 0.9710\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 12s 673ms/step - loss: 0.1026 - accuracy: 0.9721\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.0895 - accuracy: 0.9735\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 21s 1s/step - loss: 0.1164 - accuracy: 0.9712\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 17s 925ms/step - loss: 0.0864 - accuracy: 0.9759\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 11s 591ms/step - loss: 0.1094 - accuracy: 0.9733\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 11s 600ms/step - loss: 0.0795 - accuracy: 0.9757\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 11s 608ms/step - loss: 0.1101 - accuracy: 0.9706\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 11s 602ms/step - loss: 0.0793 - accuracy: 0.9757\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.1005 - accuracy: 0.9741\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 11s 609ms/step - loss: 0.0889 - accuracy: 0.9748\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 11s 603ms/step - loss: 0.1366 - accuracy: 0.9708\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 11s 629ms/step - loss: 0.0678 - accuracy: 0.9811\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 11s 606ms/step - loss: 0.0705 - accuracy: 0.9790\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 11s 612ms/step - loss: 0.0644 - accuracy: 0.9799\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 11s 618ms/step - loss: 0.0734 - accuracy: 0.9784\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 11s 621ms/step - loss: 0.0582 - accuracy: 0.9831\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 10s 532ms/step - loss: 0.0802 - accuracy: 0.9775\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 0.0536 - accuracy: 0.9831\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 9s 515ms/step - loss: 0.0546 - accuracy: 0.9835\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 9s 521ms/step - loss: 0.0507 - accuracy: 0.9848\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 0.0522 - accuracy: 0.9822\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 9s 507ms/step - loss: 0.0432 - accuracy: 0.9853\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 10s 533ms/step - loss: 0.0641 - accuracy: 0.9811\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 0.0488 - accuracy: 0.9866\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 9s 526ms/step - loss: 0.0535 - accuracy: 0.9835\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 9s 528ms/step - loss: 0.0491 - accuracy: 0.9853\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 9s 523ms/step - loss: 0.0729 - accuracy: 0.9793\n",
      "36/36 [==============================] - 1s 20ms/step\n",
      "\n",
      "Best Threshold=1.9423590856604278e-05, G-Mean=0.8783412700898934\n",
      "TP=37, FP=89, TN=989, FN=7, Sens=0.8409, Spec=0.9174, Acc=0.9144, MCC=0.4662, AUC=0.9158, F1=0.4353, Prec=0.2937, Recall=0.8409\n",
      "\n",
      "4 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4487, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4487, 2)\n",
      "The shape of validation dataset of cross validation: (1121, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1121, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 11s 568ms/step - loss: 0.3028 - accuracy: 0.8518\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 11s 590ms/step - loss: 0.7666 - accuracy: 0.9563\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 10s 567ms/step - loss: 0.2889 - accuracy: 0.9563\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 12s 651ms/step - loss: 0.2034 - accuracy: 0.9563\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 12s 652ms/step - loss: 0.1701 - accuracy: 0.9563\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 12s 651ms/step - loss: 0.1716 - accuracy: 0.9563\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 12s 656ms/step - loss: 0.1600 - accuracy: 0.9563\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 12s 670ms/step - loss: 0.1824 - accuracy: 0.9563\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 12s 664ms/step - loss: 0.1739 - accuracy: 0.9563\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 12s 654ms/step - loss: 0.1547 - accuracy: 0.9572\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 11s 589ms/step - loss: 0.1536 - accuracy: 0.9590\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 10s 565ms/step - loss: 0.1537 - accuracy: 0.9581\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 0.1413 - accuracy: 0.9617\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 9s 508ms/step - loss: 0.1634 - accuracy: 0.9608\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 0.1331 - accuracy: 0.9639\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 11s 615ms/step - loss: 0.1365 - accuracy: 0.9657\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 11s 598ms/step - loss: 0.1184 - accuracy: 0.9677\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 10s 527ms/step - loss: 0.1101 - accuracy: 0.9695\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 9s 509ms/step - loss: 0.1529 - accuracy: 0.9666\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 9s 501ms/step - loss: 0.1170 - accuracy: 0.9695\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 10s 574ms/step - loss: 0.1226 - accuracy: 0.9695\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 10s 537ms/step - loss: 0.1047 - accuracy: 0.9708\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 0.1129 - accuracy: 0.9699\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 9s 520ms/step - loss: 0.1228 - accuracy: 0.9710\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 9s 514ms/step - loss: 0.1012 - accuracy: 0.9719\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 10s 542ms/step - loss: 0.1488 - accuracy: 0.9672\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 10s 563ms/step - loss: 0.1061 - accuracy: 0.9735\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 11s 590ms/step - loss: 0.0768 - accuracy: 0.9755\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 12s 647ms/step - loss: 0.1211 - accuracy: 0.9697\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 12s 647ms/step - loss: 0.1068 - accuracy: 0.9728\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 12s 634ms/step - loss: 0.1035 - accuracy: 0.9737\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 11s 627ms/step - loss: 0.1513 - accuracy: 0.9706\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 11s 622ms/step - loss: 0.0801 - accuracy: 0.9768\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 12s 641ms/step - loss: 0.0985 - accuracy: 0.9737\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 11s 631ms/step - loss: 0.0774 - accuracy: 0.9770\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 11s 622ms/step - loss: 0.0848 - accuracy: 0.9775\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.0734 - accuracy: 0.9802\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 12s 637ms/step - loss: 0.0835 - accuracy: 0.9775\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 12s 643ms/step - loss: 0.0718 - accuracy: 0.9802\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 0.0727 - accuracy: 0.9786\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 12s 646ms/step - loss: 0.0704 - accuracy: 0.9797\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 12s 657ms/step - loss: 0.0682 - accuracy: 0.9802\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 12s 652ms/step - loss: 0.0727 - accuracy: 0.9797\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 11s 621ms/step - loss: 0.0665 - accuracy: 0.9819\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 11s 583ms/step - loss: 0.0721 - accuracy: 0.9799\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 10s 581ms/step - loss: 0.0776 - accuracy: 0.9799\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 0.0670 - accuracy: 0.9802\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 10s 552ms/step - loss: 0.0736 - accuracy: 0.9802\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 11s 588ms/step - loss: 0.0497 - accuracy: 0.9869\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 10s 564ms/step - loss: 0.0542 - accuracy: 0.9862\n",
      "36/36 [==============================] - 1s 22ms/step\n",
      "\n",
      "Best Threshold=0.0009477657731622458, G-Mean=0.836635341564193\n",
      "TP=37, FP=160, TN=916, FN=8, Sens=0.8222, Spec=0.8513, Acc=0.8501, MCC=0.3474, AUC=0.8927, F1=0.3058, Prec=0.1878, Recall=0.8222\n",
      "\n",
      "5 / 5\n",
      "\n",
      "The shape of training dataset of cross validation: (4487, 1, 35, 1024)\n",
      "The shape of training label of cross validation: (4487, 2)\n",
      "The shape of validation dataset of cross validation: (1121, 1, 35, 1024)\n",
      "The shape of validation label of cross validation: (1121, 2)\n",
      "\n",
      "\n",
      "Model: \"deep_scan_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 1, 32, 64)         262208    \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 1, 28, 64)         524352    \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 1, 20, 64)         1048640   \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               49408     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,885,122\n",
      "Trainable params: 1,885,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 11s 599ms/step - loss: 0.2622 - accuracy: 0.9392\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 12s 656ms/step - loss: 0.3336 - accuracy: 0.9588\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 11s 599ms/step - loss: 0.3118 - accuracy: 0.9588\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 10s 564ms/step - loss: 0.3077 - accuracy: 0.9588\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 10s 548ms/step - loss: 0.1610 - accuracy: 0.9585\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 0.1929 - accuracy: 0.9599\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 12s 638ms/step - loss: 0.1428 - accuracy: 0.9626\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 10s 555ms/step - loss: 0.1364 - accuracy: 0.9641\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 11s 619ms/step - loss: 0.1752 - accuracy: 0.9639\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 11s 619ms/step - loss: 0.1165 - accuracy: 0.9704\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 11s 616ms/step - loss: 0.1088 - accuracy: 0.9715\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 0.1559 - accuracy: 0.9666\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 10s 528ms/step - loss: 0.1003 - accuracy: 0.9713\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 9s 515ms/step - loss: 0.1022 - accuracy: 0.9735\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 10s 530ms/step - loss: 0.1363 - accuracy: 0.9706\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 11s 601ms/step - loss: 0.0884 - accuracy: 0.9762\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 10s 543ms/step - loss: 0.0979 - accuracy: 0.9744\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 0.0942 - accuracy: 0.9759\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 11s 604ms/step - loss: 0.0870 - accuracy: 0.9775\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 11s 622ms/step - loss: 0.1388 - accuracy: 0.9699\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 10s 572ms/step - loss: 0.0875 - accuracy: 0.9770\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 0.1119 - accuracy: 0.9726\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 10s 531ms/step - loss: 0.0703 - accuracy: 0.9811\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 10s 585ms/step - loss: 0.0790 - accuracy: 0.9788\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 11s 616ms/step - loss: 0.0679 - accuracy: 0.9822\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 10s 554ms/step - loss: 0.0605 - accuracy: 0.9846\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 10s 540ms/step - loss: 0.0849 - accuracy: 0.9764\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 10s 547ms/step - loss: 0.0613 - accuracy: 0.9837\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 10s 529ms/step - loss: 0.0695 - accuracy: 0.9791\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 10s 541ms/step - loss: 0.0550 - accuracy: 0.9873\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 11s 597ms/step - loss: 0.0676 - accuracy: 0.9804\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 10s 570ms/step - loss: 0.0489 - accuracy: 0.9886\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 0.0583 - accuracy: 0.9842\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 10s 564ms/step - loss: 0.0527 - accuracy: 0.9855\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 10s 570ms/step - loss: 0.0557 - accuracy: 0.9837\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 10s 550ms/step - loss: 0.0400 - accuracy: 0.9929\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 10s 553ms/step - loss: 0.0491 - accuracy: 0.9855\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 12s 665ms/step - loss: 0.0361 - accuracy: 0.9911\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 11s 617ms/step - loss: 0.0386 - accuracy: 0.9909\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 10s 586ms/step - loss: 0.0561 - accuracy: 0.9848\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 11s 587ms/step - loss: 0.0801 - accuracy: 0.9811\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 9s 506ms/step - loss: 0.0862 - accuracy: 0.9784\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 10s 561ms/step - loss: 0.0515 - accuracy: 0.9864\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 10s 539ms/step - loss: 0.0402 - accuracy: 0.9889\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 10s 545ms/step - loss: 0.0357 - accuracy: 0.9904\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 11s 607ms/step - loss: 0.0290 - accuracy: 0.9931\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 11s 614ms/step - loss: 0.0309 - accuracy: 0.9924\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 10s 575ms/step - loss: 0.0291 - accuracy: 0.9911\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 10s 544ms/step - loss: 0.0251 - accuracy: 0.9931\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 10s 549ms/step - loss: 0.0224 - accuracy: 0.9949\n",
      "36/36 [==============================] - 1s 19ms/step\n",
      "\n",
      "Best Threshold=0.0005026638973504305, G-Mean=0.868635230104022\n",
      "TP=45, FP=65, TN=1000, FN=11, Sens=0.8036, Spec=0.9390, Acc=0.9322, MCC=0.5438, AUC=0.9142, F1=0.5422, Prec=0.4091, Recall=0.8036\n",
      "\n",
      "The mean of 5-Fold cross-validation results:\n",
      "TP=39.6, FP=100.4, TN=973.0, FN=8.6, Sens=0.8233, Spec=0.9065, Acc=0.9028, MCC=0.4557, AUC=0.9086, F1=0.431, Prec=0.297, Recall=0.297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if(VALIDATION_MODE == \"cross\"):\n",
    "    # Initialize K-Fold cross-validation\n",
    "    kfold = KFold(n_splits=K_Fold, shuffle=True, random_state=2)\n",
    "    \n",
    "    results = []  # List to store results of each fold\n",
    "    i = 1  # Counter for fold number\n",
    "    \n",
    "    # Iterate over each split of the dataset\n",
    "    for train_index, test_index in kfold.split(x_train):\n",
    "        print(f\"{i} / {K_Fold}\\n\")\n",
    "        \n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = x_train[train_index], x_train[test_index]\n",
    "        Y_train, Y_test = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        # Print the shapes of the training and testing datasets\n",
    "        print(\"The shape of training dataset of cross validation:\", X_train.shape)\n",
    "        print(\"The shape of training label of cross validation:\", Y_train.shape)\n",
    "        print(\"The shape of validation dataset of cross validation:\", X_test.shape)\n",
    "        print(\"The shape of validation label of cross validation:\", Y_test.shape)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Create a data generator for the training data\n",
    "        generator = DataGenerator(X_train, Y_train, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Initialize the DeepScan model\n",
    "        model = DeepScan(\n",
    "            num_filters=NUM_FILTER,\n",
    "            num_hidden=NUM_HIDDEN,\n",
    "            window_sizes=WINDOW_SIZES\n",
    "        )\n",
    "        \n",
    "        # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Build the model with the input shape of the training data\n",
    "        model.build(input_shape=X_train.shape)\n",
    "        \n",
    "        # Print the model summary\n",
    "        model.summary()\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)],\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Test the model on the validation set and get performance metrics\n",
    "        TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall = model_test(model, X_test, Y_test)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        results.append([TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall])\n",
    "        \n",
    "        # Increment the fold counter\n",
    "        i += 1\n",
    "        \n",
    "        # Clear the training and testing data from memory\n",
    "        del X_train\n",
    "        del X_test\n",
    "        del Y_train\n",
    "        del Y_test\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate the mean results across all folds\n",
    "    mean_results = np.mean(results, axis=0)\n",
    "    \n",
    "    # Print the mean results of the cross-validation\n",
    "    print(f\"The mean of {K_Fold}-Fold cross-validation results:\")\n",
    "    print(f'TP={mean_results[0]:.4}, FP={mean_results[1]:.4}, TN={mean_results[2]:.4}, FN={mean_results[3]:.4}, '\n",
    "          f'Sens={mean_results[4]:.4}, Spec={mean_results[5]:.4}, Acc={mean_results[6]:.4}, MCC={mean_results[7]:.4}, AUC={mean_results[8]:.4}, F1={mean_results[9]:.4}, Prec={mean_results[10]:.4}, Recall={mean_results[10]:.4}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "19b93222",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_MODE == \"independent\"):\n",
    "    # Create a data generator for the training data\n",
    "    generator = DataGenerator(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize the DeepScan model\n",
    "    model = DeepScan(\n",
    "        num_filters=NUM_FILTER,\n",
    "        num_hidden=NUM_HIDDEN,\n",
    "        window_sizes=WINDOW_SIZES\n",
    "    )\n",
    "    \n",
    "    # Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Build the model with the input shape of the training data\n",
    "    model.build(input_shape=x_train.shape)\n",
    "    \n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        generator,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    # Test the model on the independent test set and get performance metrics\n",
    "    TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC, F1, Prec, Recall = model_test(model, x_test, y_test)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}, F1={F1:.4f}, Prec={Prec:.4f}, Recall={Recall:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e32a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
